# **Chaos Engineering sobre Kubernetes con Litmus**

- [**Chaos Engineering sobre Kubernetes con Litmus**](#chaos-engineering-sobre-kubernetes-con-litmus)
  - [**Sobre *Chaos Engineering***](#sobre-chaos-engineering)
  - [**Componentes Litmus**](#componentes-litmus)
  - [**Workshop**](#workshop)
    - [**Preparaci贸n de consola**](#preparaci贸n-de-consola)
    - [**Creaci贸n de entorno de pruebas con minikube**](#creaci贸n-de-entorno-de-pruebas-con-minikube)
    - [**Creaci贸n de namespaces**](#creaci贸n-de-namespaces)
    - [**Despliegue de aplicaci贸n de test**](#despliegue-de-aplicaci贸n-de-test)
    - [**Instalaci贸n *Chaos Experiments***](#instalaci贸n-chaos-experiments)
    - [**Despliegue servicios monitorizaci贸n: Prometheus + Grafana**](#despliegue-servicios-monitorizaci贸n-prometheus--grafana)
    - [**Creaci贸n de anotaci贸n "litmuschaos"**](#creaci贸n-de-anotaci贸n-litmuschaos)
    - [**Detalle componetes de un experimento**](#detalle-componetes-de-un-experimento)
      - [**Service Account, Role y RoleBinding**](#service-account-role-y-rolebinding)
      - [**Definici贸n ChaosEngine**](#definici贸n-chaosengine)
        - [**Especificaciones generales**](#especificaciones-generales)
        - [**Especificaciones de componentes**](#especificaciones-de-componentes)
        - [**Especificaciones de pruebas**](#especificaciones-de-pruebas)
    - [**Ejecuci贸n de experimentos**](#ejecuci贸n-de-experimentos)
      - [**Container Kill**](#container-kill)
      - [**pod-autoscaler**](#pod-autoscaler)
      - [**Pod CPU Hog**](#pod-cpu-hog)
      - [**Pod Memory Hog**](#pod-memory-hog)
    - [**Planificaci贸n de experimentos**](#planificaci贸n-de-experimentos)
    - [**Litmus UI Portal**](#litmus-ui-portal)
  - [**Gu铆a Litmus para desarrolladores**](#gu铆a-litmus-para-desarrolladores)
  - [***Chaos Engineering* en despliegue Continuo**](#chaos-engineering-en-despliegue-continuo)
  - [**Consideraciones finales**](#consideraciones-finales)
  - [**Sobre el autor**](#sobre-el-autor)
  

## **Sobre *Chaos Engineering***

## **Componentes Litmus**

* **ChaosEngine**
* **ChaosExperiment**
* **ChaosSchedule**
* **ChaosResult**
* **Litmus Probes**

## **Workshop**

### **Preparaci贸n de consola**

Recomendamos abrir una consola y crear 4 paneles:

1. Estado del sistema (ejecutar htop)
2. Panel principal (ejecutaremos todo el contenido del workshop)
3. Monitorizaci贸n de la aplicaci贸n
4. Monitorizaci贸n de recursos kubernetes
   
![Console tabs](./docs/img/console-tabs.png)

### **Creaci贸n de entorno de pruebas con minikube**

```bash
# install kubectl
curl -Ls "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" --output /tmp/kubectl
sudo install /tmp/kubectl /usr/local/bin/kubectl
kubectl version --client

# install minikube
curl -Ls https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 --output /tmp/minikube-linux-amd64
sudo install /tmp/minikube-linux-amd64 /usr/local/bin/minikube
minikube version

# starting minikube
minikube start --cpus 2 --memory 4096

# enabled ingress & metrics servers
minikube addons enable ingress
minikube addons enable metrics-server

# enabled tunnel & dashboard
minikube tunnel > /dev/null &
minikube dashboard > /dev/null &
```

### **Creaci贸n de namespaces**
```bash
# create namespace testing
kubectl apply -f src/base/testing-ns.yaml

# create namespace litmus
kubectl apply -f src/base/litmus-ns.yaml

# create namespace monitoring (prometheus + grafana)
kubectl apply -f src/base/monitoring-ns.yaml

TESTING_NAMESPACE="testing"
LITMUS_NAMESPACE="litmus"
MONITORING_NAMESPACE="monitoring"
```

### **Despliegue de aplicaci贸n de test**

Desplegamos una aplicaci贸n de test para poder ejecutar los experimentos de litmus.
* **nginx-deployment.yaml**: creaci贸n de despliegue "app-sample", con recursos de cpu/memoria "limits"/"request" y configuraci贸n de "readinessProbe". Exponemos el servicio en el puerto 8080 a trav茅s de un balanceador. 
* **ngix-hpa.yaml**: creaci贸n de *Horizontal Pod Autoscaler* (min 2 r茅plicas / max 10 r茅plicas)

```bash
# deployment
kubectl apply -f src/nginx/nginx-deployment.yaml --namespace="${TESTING_NAMESPACE}"

# enable hpa
kubectl apply -f src/nginx/ngix-hpa.yaml --namespace="${TESTING_NAMESPACE}"

# expose service 
kubectl expose deployment app-sample --type=LoadBalancer --port=8080  -n "${TESTING_NAMESPACE}"

# get pods
kubectl get pods -n "${TESTING_NAMESPACE}"

NAME                          READY   STATUS    RESTARTS   AGE
app-sample-7ff489dbd5-82ppw   1/1     Running   0          45m
app-sample-7ff489dbd5-jg9vh   1/1     Running   0          45m
```

:information_source: En el panel 3 ejecutar el siguiente c贸digo para recibir la respuesta de la aplicaci贸n. 
```bash
TESTING_NAMESPACE='testing'
URL_SERVICE=$(minikube service app-sample --url -n "${TESTING_NAMESPACE}")
while true; do sleep 5; curl --connect-timeout 3 ${URL_SERVICE}; echo -e ' '$(date);done
```

![Console tabs 3](./docs/img/console-tabs-3.png)

### **Instalaci贸n *Chaos Experiments***

```bash
# litmus operator & experiments
kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.13.0.yaml -n "${LITMUS_NAMESPACE}"

kubectl apply -f https://hub.litmuschaos.io/api/chaos/1.13.0\?file\=charts/generic/experiments.yaml -n "${TESTING_NAMESPACE}"
```

```bash
kubectl get chaosexperiments -n "${TESTING_NAMESPACE}"

NAME                      AGE
container-kill            6s
disk-fill                 6s
disk-loss                 6s
docker-service-kill       6s
k8-pod-delete             6s
k8-service-kill           6s
kubelet-service-kill      6s
node-cpu-hog              6s
node-drain                6s
node-io-stress            6s
node-memory-hog           6s
node-poweroff             6s
node-restart              6s
node-taint                6s
pod-autoscaler            6s
pod-cpu-hog               6s
pod-delete                6s
pod-io-stress             6s
pod-memory-hog            6s
pod-network-corruption    6s
pod-network-duplication   6s
pod-network-latency       6s
pod-network-loss          6s
```

### **Despliegue servicios monitorizaci贸n: Prometheus + Grafana**

```bash
# create namespace litmus
kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/prometheus/prometheus-operator/

kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/metrics-exporters-with-service-monitors/node-exporter/

kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/metrics-exporters-with-service-monitors/kube-state-metrics/

kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/alert-manager-with-service-monitor/

kubectl -n ${LITMUS_NAMESPACE} apply -f src/litmus/monitoring/utils/metrics-exporters-with-service-monitors/litmus-metrics/chaos-exporter/

kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/prometheus/prometheus-configuration/

kubectl -n ${MONITORING_NAMESPACE} apply -f src/litmus/monitoring/utils/grafana/

minikube service grafana -n ${MONITORING_NAMESPACE} > /dev/null &
```

### **Creaci贸n de anotaci贸n "litmuschaos"**

Ya tenemos todos los componentes de litmus desplegados. 

```bash
# add annotate (enable chaos)
kubectl annotate deploy/app-sample litmuschaos.io/chaos="true" -n "${TESTING_NAMESPACE}"
``` 

```bash
kubectl describe deploy/app-sample -n "${TESTING_NAMESPACE}"

Name:                   app-sample
Namespace:              testing
CreationTimestamp:      Mon, 29 Mar 2021 09:35:53 +0200
Labels:                 app=app-sample
                        app.kubernetes.io/name=app-sample
Annotations:            deployment.kubernetes.io/revision: 1
                        litmuschaos.io/chaos: true # <-- HABILITAMOS EXPERIMENTOS
Selector:               app.kubernetes.io/name=app-sample
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
```

### **Detalle componetes de un experimento**

#### **Service Account, Role y RoleBinding**

Cada experimento debe tener asociado un ServiceAccount, un Role para definir permisos y un RoleBinding para relacionar el ServiceAccount/Role.

Pod茅is encontrar todas las definiciones dentro de *src/litmus/nombre-experimento/nombre-experimento-sa.yaml*

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: container-kill-sa
  namespace: testing
  labels:
    name: container-kill-sa
    app.kubernetes.io/part-of: litmus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: container-kill-sa
  namespace: testing
  labels:
    name: container-kill-sa
    app.kubernetes.io/part-of: litmus
rules:
  - apiGroups: [""]
    resources:
      ["pods", "pods/exec", "pods/log", "events", "replicationcontrollers"]
    verbs:
      ["create", "list", "get", "patch", "update", "delete", "deletecollection"]
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "list", "get", "delete", "deletecollection"]
  - apiGroups: ["apps"]
    resources: ["deployments", "statefulsets", "daemonsets", "replicasets"]
    verbs: ["list", "get"]
  - apiGroups: ["apps.openshift.io"]
    resources: ["deploymentconfigs"]
    verbs: ["list", "get"]
  - apiGroups: ["argoproj.io"]
    resources: ["rollouts"]
    verbs: ["list", "get"]
  - apiGroups: ["litmuschaos.io"]
    resources: ["chaosengines", "chaosexperiments", "chaosresults"]
    verbs: ["create", "list", "get", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: container-kill-sa
  namespace: testing
  labels:
    name: container-kill-sa
    app.kubernetes.io/part-of: litmus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: container-kill-sa
subjects:
  - kind: ServiceAccount
    name: container-kill-sa
    namespace: testing
```

#### **Definici贸n ChaosEngine**

Para facilitar la comprensi贸n, hemos dividido en 3 secciones el contenido de un experimiento. Pod茅is encontrar todas las definiciones dentro de *src/litmus/nombre-experimento/chaos-engine-*.yaml*


##### **Especificaciones generales**

En esta secci贸n especificaremos atributos comunes a todos los experimentos. Para este workshop, debido a que estamos realizando los experimentos contra un 煤nico deployment, el 煤nico atributo que cambiar谩 entre experimentos es "chaosServiceAccount".

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: app-sample-chaos # Nombre del chaos-engine
  namespace: testing     # Namespace de testing
spec:
  annotationCheck: "true" # Hemos creado una anotaci贸n en nuestro deployment app-sample. Con la propiedad marcada a "true" indicamos que aplicarmeos el experimento a este despliegue.

  engineState: "active"   # Activaci贸n/desactivaci贸n de experimento

  appinfo:                # En esta secci贸n proporcionamos la informaci贸n de nuestro deployment.
    appns: "testing"      # Namespace donde se localiza
    applabel: "app.kubernetes.io/name=app-sample" # Etiqueta asociada a nuestro deployment
    appkind: "deployment" # Tipo de recurso (s贸lo admite deployment, lo que afectar谩 a todos los pods)

  chaosServiceAccount: container-kill-sa # Nombre del service account (creado en el paso anterior)
  monitoring: true       # si queremos activar la monitorizaci贸n (prometheus o similares)
  jobCleanUpPolicy: "delete" # Permite controlar la limpieza de recursos tras la ejecuci贸n. Especificar "retain" para debug.
```

##### **Especificaciones de componentes**
En esta secci贸n definiremos las variables de entorno propias de cada experimento. Las variables "CHAOS_INTERVAL" y "TOTAL_CHAOS_DURATION" son comunes a todos los experimentos.
```yaml
  experiments:
    - name: container-kill # Nombre del experimento
      spec:
        components:
          env:
            # Intervalo (segundos) por cada iteraci贸n
            - name: CHAOS_INTERVAL
              value: "10"

            # Tiempo total (segundos) que durar谩 el experimento
            - name: TOTAL_CHAOS_DURATION
              value: "60"
``` 

##### **Especificaciones de pruebas**
En esta secci贸n se informan los atributos para las pruebs de validaci贸n. El resultado del experiment depender谩 del cumplimiento de la validaci贸n especificada. 

En el siguiente [enlace](https://docs.litmuschaos.io/docs/litmus-probe/) podr茅is consultar los tipo de pruebas disponibles.

```yaml
        probe:
          - name: "check-frontend-access-url" # Nombre de prueba
            type: "httpProbe"                 # Petici贸n de tipo HTTP(S). Alternativas: cmdProbe, k8sProbe, promProbe.
            httpProbe/inputs:                  
              url: "http://app-sample.testing.svc.cluster.local:8080" # URL a validar
              insecureSkipVerify: false                               # Permitir HTTP sin TLS
              method:
                get:                          # Petici贸n tipo GET
                  criteria: ==                # Criterio a evaluar
                  responseCode: "200"         # Respuesta a evaluar
            mode: "Continuous"                # La prueba se ejecuta de forma continua (alternativas: SoT, EoT, Edge, OnChaos)
            runProperties:
              probeTimeout: 5                 # N煤mero de segundos para timeout en la petici贸n
              interval: 5                     # Intervalo (segundos) entre re-intentos
              retry: 1                        # N煤mero de re-intento antes de dar por fallida la validaci贸n   
              probePollingInterval: 2         # Intervalo (segundos) entre peticiones

```

### **Ejecuci贸n de experimentos**

#### **Container Kill**

- **Descripci贸n:** Aborta la ejecuci贸n del servicio docker dentro de un pod. La selecci贸n del pod es aleatoria.

- **Informaci贸n oficial del experimento:** [enlace](https://docs.litmuschaos.io/docs/container-kill/)
  
- **Criterio de entrada:** 2 pods de app-sample en estado "Running"
  
  ```bash
    kubectl get pods -n "${TESTING_NAMESPACE}"

    NAME                          READY   STATUS    RESTARTS   AGE
    app-sample-7ff489dbd5-82ppw   1/1     Running   0          9h
    app-sample-7ff489dbd5-jg9vh   1/1     Running   0          9h
  ```

- **Par谩metros de entrada experimento:**

    ```yaml
    experiments:
        - name: container-kill
        spec:
            components:
            env:
                # provide the chaos interval
                - name: CHAOS_INTERVAL
                value: "10"

                # provide the total chaos duration
                - name: TOTAL_CHAOS_DURATION
                value: "20"

                - name: CONTAINER_RUNTIME
                value: "docker"

                - name: SOCKET_PATH
                value: "/var/run/docker.sock"
    ```

- **Hip贸tesis:** Tenemos dos pods escuchando por el 8080 tras un balanceador. Nuestro deployment tiene readinessProbe con periodSeconds=1 y failureThreshold=1. Si uno de los pods deja de responder, el balanceador deja de enviar tr谩fico a ese pod y debe responder el otro. Hemos establecido el healthcheck del experimento cada 5s (tiempo m谩ximo de respuesta aceptable) atacando directamente contra el balanceador, por lo que no deber铆amos de tener p茅rdida de servicio en ning煤n momento. 

- **Creaci贸n de SA, Role y RoleBinding**

    ```bash
    kubectl apply -f src/litmus/kill-container/kill-container-sa.yaml -n "${TESTING_NAMESPACE}"
    ```

- **Ejecuci贸n de experimento**

    ```bash
    kubectl apply -f src/litmus/kill-container/chaos-engine-kill-container.yaml  -n "${TESTING_NAMESPACE}"
    
    # Awaited -> Pass/Fail
    watch -n 1 kubectl get chaosresult app-sample-chaos-container-kill -n "${TESTING_NAMESPACE}" -o jsonpath="{.status.experimentstatus.verdict}"
    ```

- **Observaciones:** durante el experimento observamos 2 reinicios de pod con transici贸n "Running" -> "Error" -> "Running". 

- **Validaci贸n:** Peticiones get al balanceador con respuesta 200.

    ```yaml
    probe:
    - name: "check-frontend-access-url"
        type: "httpProbe"
        httpProbe/inputs:
        url: "http://app-sample.testing.svc.cluster.local:8080"
        insecureSkipVerify: false
        method:
            get:
            criteria: ==
            responseCode: "200"
        mode: "Continuous"
        runProperties:
        probeTimeout: 5
        interval: 5
        retry: 1
        probePollingInterval: 2
    ```

- **Resultado:** resultado "Pass" (dos pods en estado "Running", sin p茅rdida de servicio durante la duraci贸n del experimento)

    ```bash
    $ kubectl describe chaosresult app-sample-chaos-container-kill -n "${TESTING_NAMESPACE}" 

    Spec:
        Engine:      app-sample-chaos
        Experiment:  container-kill
    Status:
        Experimentstatus:
            Fail Step:                 N/A
            Phase:                     Completed
            Probe Success Percentage:  100
            Verdict:                   Pass
    History:
        Failed Runs:   0
        Passed Runs:   6
        Stopped Runs:  0
    Probe Status:
        Name:  check-frontend-access-url
        Status:
            Continuous:  Passed 
        Type:            httpProbe
    Events:
        Type    Reason   Age    From                         Message
        ----    ------   ----   ----                         -------
        Normal  Awaited  3m52s  container-kill-fs68ut-p9ps7  experiment: container-kill, Result: Awaited
        Normal  Pass     2m7s   container-kill-fs68ut-p9ps7  experiment: container-kill, Result: Pass
    
    
    $ kubectl get pods -n testing
    
    NAME                          READY   STATUS    RESTARTS   AGE
    app-sample-7ff489dbd5-82ppw   1/1     Running   2          9h
    app-sample-7ff489dbd5-jg9vh   1/1     Running   0          9h
    ```





#### **pod-autoscaler**

- **Descripci贸n:** permite escalar las r茅plicas para testear el autoescalado en el nodo.

- **Informaci贸n oficial del experimento:** [enlace](https://docs.litmuschaos.io/docs/pod-autoscaler/)
  
- **Criterio de entrada:** 2 pods de app-sample en estado "Running"
  
  ```bash
    kubectl get pods -n "${TESTING_NAMESPACE}"

  ```

- **Par谩metros de entrada experimento:**

    ```yaml
    ```

- **Hip贸tesis:** Tenemos dos pods escuchando por el 8080 tras un balanceador. Nuestro deployment tiene readinessProbe con periodSeconds=1 y failureThreshold=1. Si uno de los pods deja de responder, el balanceador deja de enviar tr谩fico a ese pod y debe responder el otro. Hemos establecido el healthcheck del experimento cada 5s (tiempo m谩ximo de respuesta aceptable) atacando directamente contra el balanceador, por lo que no deber铆amos de tener p茅rdida de servicio en ning煤n momento. 

- **Creaci贸n de SA, Role y RoleBinding**

    ```bash
    
    ```

- **Ejecuci贸n de experimento**

    ```bash
    ```

- **Observaciones:**

- **Validaci贸n:** Peticiones get al balanceador con respuesta 200.

    ```yaml
    probe:
    - name: "check-frontend-access-url"
        type: "httpProbe"
        httpProbe/inputs:
        url: "http://app-sample.testing.svc.cluster.local:8080"
        insecureSkipVerify: false
        method:
            get:
            criteria: ==
            responseCode: "200"
        mode: "Continuous"
        runProperties:
        probeTimeout: 5
        interval: 5
        retry: 1
        probePollingInterval: 2
    ```

- **Resultado:** resultado "Pass" (dos pods en estado "Running", sin p茅rdida de servicio durante la duraci贸n del experimento)

    ```bash
    $ kubectl describe chaosresult app-sample-chaos-container-kill -n "${TESTING_NAMESPACE}" 

    $ kubectl get pods -n testing
    
    ```

#### **Pod CPU Hog**

- **Descripci贸n:**

- **Informaci贸n oficial del experimento:** [enlace](https://docs.litmuschaos.io/docs/pod-memory-hog/)
  
- **Criterio de entrada:** 2 pods de app-sample en estado "Running"
  
  ```bash
    kubectl get pods -n "${TESTING_NAMESPACE}"

  ```

- **Par谩metros de entrada experimento:**

    ```yaml
    ```

- **Hip贸tesis:** Tenemos dos pods escuchando por el 8080 tras un balanceador. Nuestro deployment tiene readinessProbe con periodSeconds=1 y failureThreshold=1. Si uno de los pods deja de responder, el balanceador deja de enviar tr谩fico a ese pod y debe responder el otro. Hemos establecido el healthcheck del experimento cada 5s (tiempo m谩ximo de respuesta aceptable) atacando directamente contra el balanceador, por lo que no deber铆amos de tener p茅rdida de servicio en ning煤n momento. 

- **Creaci贸n de SA, Role y RoleBinding**

    ```bash
    
    ```

- **Ejecuci贸n de experimento**

    ```bash
    ```

- **Observaciones:**

- **Validaci贸n:** Peticiones get al balanceador con respuesta 200.

    ```yaml
    probe:
    - name: "check-frontend-access-url"
        type: "httpProbe"
        httpProbe/inputs:
        url: "http://app-sample.testing.svc.cluster.local:8080"
        insecureSkipVerify: false
        method:
            get:
            criteria: ==
            responseCode: "200"
        mode: "Continuous"
        runProperties:
        probeTimeout: 5
        interval: 5
        retry: 1
        probePollingInterval: 2
    ```

- **Resultado:** resultado "Pass" (dos pods en estado "Running", sin p茅rdida de servicio durante la duraci贸n del experimento)

    ```bash
    $ kubectl describe chaosresult app-sample-chaos-container-kill -n "${TESTING_NAMESPACE}" 

    $ kubectl get pods -n testing
    
    ```

#### **Pod Memory Hog**

- **Descripci贸n:**

- **Informaci贸n oficial del experimento:** [enlace](https://docs.litmuschaos.io/docs/pod-memory-hog/)
  
- **Criterio de entrada:** 2 pods de app-sample en estado "Running"
  
  ```bash
    kubectl get pods -n "${TESTING_NAMESPACE}"

  ```

- **Par谩metros de entrada experimento:**

    ```yaml
    ```

- **Hip贸tesis:** Tenemos dos pods escuchando por el 8080 tras un balanceador. Nuestro deployment tiene readinessProbe con periodSeconds=1 y failureThreshold=1. Si uno de los pods deja de responder, el balanceador deja de enviar tr谩fico a ese pod y debe responder el otro. Hemos establecido el healthcheck del experimento cada 5s (tiempo m谩ximo de respuesta aceptable) atacando directamente contra el balanceador, por lo que no deber铆amos de tener p茅rdida de servicio en ning煤n momento. 

- **Creaci贸n de SA, Role y RoleBinding**

    ```bash
    
    ```

- **Ejecuci贸n de experimento**

    ```bash
    ```

- **Observaciones:**

- **Validaci贸n:** Peticiones get al balanceador con respuesta 200.

    ```yaml
    probe:
    - name: "check-frontend-access-url"
        type: "httpProbe"
        httpProbe/inputs:
        url: "http://app-sample.testing.svc.cluster.local:8080"
        insecureSkipVerify: false
        method:
            get:
            criteria: ==
            responseCode: "200"
        mode: "Continuous"
        runProperties:
        probeTimeout: 5
        interval: 5
        retry: 1
        probePollingInterval: 2
    ```

- **Resultado:** resultado "Pass" (dos pods en estado "Running", sin p茅rdida de servicio durante la duraci贸n del experimento)

    ```bash
    $ kubectl describe chaosresult app-sample-chaos-container-kill -n "${TESTING_NAMESPACE}" 

    $ kubectl get pods -n testing
    
    ```

### **Planificaci贸n de experimentos**

### **Litmus UI Portal**

Litmus dispone de un portal para poder realizar experimentos sin necesidad utilizar la consola. Dispone de las siguientes funcionalidades:

- Gesti贸n de workflows: dispone de todos los experimentos pre-cargados listos para ejecutar en tu k8s.
- MyHubs: permite conectar a repositorios p煤blicos/privados para hacer uso de tus propios experimentos.
- Analytics: permite visualizar las ejecuciones de tus experimentos, as铆 como estad铆sticas sobre los mismos. Adem谩s, permite conectar a otros DataSources como Prometheus.
- Gesti贸n de equipos y usuarios. 


```bash
# install litmus portal
kubectl apply -f src/litmus/portal/portal.yaml

# kubectl apply -f src/litmus/portal.yaml
minikube service litmusportal-frontend-service -n  ${LITMUS_NAMESPACE} > /dev/null &
```

![Litmus Portal](./docs/img/litmus-portal.png)

## **Gu铆a Litmus para desarrolladores** 

En la actualidad, litmus dispone de 48 experimentos a trav茅s de [Litmus ChaosHub](https://hub.litmuschaos.io/). Est谩n desarrollados principalmente en Go, aunque disponen de una SDK para python y ansible. 

Los experimentos tienen una estructura bien definida (pre-checks, chaos-injection, litmus-probes, post-checks y result-updates) y es viable desarrollar experimentos que se ajusten a tus necesidades. 

En este [enlace](https://docs.litmuschaos.io/docs/devguide/) encontrar茅is toda la informaci贸n para desarrolladores.


## ***Chaos Engineering* en despliegue Continuo**

En este workshop 

## **Consideraciones finales** 

Elaborar un plan de pruebas que te permita conocer el comportamiento 铆ntegro de tu sistema puede ser una tarea relativamente compleja.En este workshop hemos trabajado la inyecci贸n de errores en kubernetes utilizando un 煤nico servicio bajo un 煤nico nodo, pero los sistemas distribuidos suelen ser mucho m谩s complejos: decenas de microservicios ejecutando en m煤ltiples nodos de k8s sobre infraestructura cloud, con alta disponibilidad implementada con multi-AZ/multi-region, comunicaciones con on-premise, etc.



## **Sobre el autor**







  